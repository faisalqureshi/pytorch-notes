{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch dataset\n",
    "Faisal Z. Qureshi     \n",
    "http://vclab.science.uoit.ca\n",
    "\n",
    "Using `torch.utils.data.dataset.random_split` to split a given dataset into more than one (sub)datasets.  This is handy since it can be used to create training, validation, and test sets.\n",
    "\n",
    "- Use training set for training purposes.<br>\n",
    "- Use validation set during training to check *underfitting* and *overfitting*.  If the model does very well on training data, but does poorly on validation data then the model is overfitting, i.e., the model will not generalize well.<br>\n",
    "- Neither validation nor test data is ever used for training purposes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint as pp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "get_ipython().run_line_magic('matplotlib', 'inline')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "n_samples = 1000\n",
    "x, y = datasets.make_blobs(n_samples=n_samples, n_features=2, centers=[(0,5),(4,0)], random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(7,7))\n",
    "plt.scatter(x[:,0], x[:,1], c=y, cmap=cm.bwr, alpha=0.4)\n",
    "plt.xlabel('$x_1$')\n",
    "plt.ylabel('$x_2$')\n",
    "plt.xlim(-3,10)\n",
    "plt.ylim(-3,10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our own dataset class.  The benefit is that we can use this class within the dataloader class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, x, y):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        sample = {\n",
    "            'feature': torch.tensor([self.x[index]], dtype=torch.float32), \n",
    "            'label': torch.tensor([self.y[index]], dtype=torch.long)}\n",
    "        return sample\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = MyDataset(x, y)\n",
    "print('length: ', len(dataset))\n",
    "for i in range(len(dataset)):\n",
    "    #pp.pprint(dataset[i])\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using `random_split` to split the data into training and testing sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import random_split\n",
    "train, test = random_split(dataset, [800, 200])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just confirming that random_split is doing what it is supposed to.  There is no overlap between the two (sub)datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('train length: ', len(train))\n",
    "x_train = np.empty((2, len(train)))\n",
    "y_train = np.empty((len(train)))\n",
    "for i in range(len(train)):\n",
    "    #pp.pprint(train[i])\n",
    "    x_train[:,i] = train[i]['feature'].numpy().reshape(2)\n",
    "    y_train[i] = train[i]['label'][0].numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('test length: ', len(test))    \n",
    "x_test = np.empty((2, len(test)))\n",
    "y_test = np.empty((len(test)))\n",
    "for i in range(len(test)):\n",
    "    #pp.pprint(test[i])\n",
    "    x_test[:,i] = test[i]['feature'].numpy().reshape(2)\n",
    "    y_test[i] = test[i]['label'][0].numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(7,7))\n",
    "plt.scatter(x_train[0,:], x_train[1,:], c=y_train, cmap=cm.bwr, alpha=0.4)\n",
    "plt.scatter(x_test[0,:], x_test[1,:], c=y_test, cmap=cm.PiYG, alpha=0.4)\n",
    "plt.xlabel('$x_1$')\n",
    "plt.ylabel('$x_2$')\n",
    "plt.xlim(-3,10)\n",
    "plt.ylim(-3,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
